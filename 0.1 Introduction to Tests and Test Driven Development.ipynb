{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing Tests in Python\n",
    "============\n",
    "Why write tests?\n",
    "- Check that your code is working\n",
    "- Add new features fearlessly without breaking old ones \n",
    "- Think strategically about desired outcomes \n",
    "\n",
    "\n",
    "\"A significant advantage of TDD is that it enables you to take small steps when writing software.\" - [Agile Data](http://agiledata.org/essays/tdd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a test?\n",
    "\n",
    "A **unit test** is a test that tests a *unit* of code, for example a function or method.\n",
    "This is different than an integration test or an end-to-end test, which both work to test a service provided by a deployed program, or an entire user experience provided by a suite of individual programs.\n",
    "\n",
    "\n",
    "Unit:\n",
    "- a small part of an application typically a function or method\n",
    "\n",
    "\n",
    "Testing:\n",
    "- run unit for some input to verify its output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Test Driven Development (TDD)?\n",
    "Test-driven development [(TDD)](https://technologyconversations.com/2013/12/20/test-driven-development-tdd-example-walkthrough/) is a software development process that relies on the repetition of a very short development cycle: first the developer writes an (initially failing) automated test case that defines a desired improvement or new function, then produces the minimum amount of code to pass that test, and finally refactors the new code to acceptable standards.\n",
    "\n",
    "The following sequence of steps is generally followed:\n",
    "\n",
    "- Add a test\n",
    "- Run all tests and see if the new one fails\n",
    "- Write some code\n",
    "- Run tests\n",
    "- Refactor code\n",
    "- Repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where does TDD fit in the Data Science Process?\n",
    "Data Science can be split into two phases: **Exploratory**, and **Production**.\n",
    "\n",
    "Tests are more for the production phase, when you're trying out features that will actually make it into your product. \n",
    "Reading material on pushing your DS model to production ASAP: http://engineering.pivotal.io/post/api-first-for-data-science/ .<br>\n",
    "That certainly doesn't mean you can't or shouldn't use tests in the exploratory phase to check that functions are working properly! You should! It just becomes more vital in the production stage.\n",
    "\n",
    "#### Some other good advice around testing (from [here](http://engineering.pivotal.io/post/test-driven-development-for-data-science/)):\n",
    "1. Start TDD once you have a fair idea of what model and features will go in production.\n",
    "2. Don’t test drive everything! Figure out the core functionalities of your code. For example while feature generation, if a feature is generated using simple count functionality of SQL, trust that SQL’s in-build count functionality is already well tested. Writing another test around that won’t add much value.\n",
    "3. Test those bits of pipeline where data is involved. The work product of a data science use case is data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks of Good Test Writing\n",
    "\n",
    "\n",
    "When writing any test, it is good to ask yourself: What is the desired outcome of the test, and the function/class it is testing?\n",
    "\n",
    "### The Assert Statement:\n",
    "- Goal: verify that the value your code produced is equal to the expected value.\n",
    "- Typically, a strict equality is used\n",
    "    - `assert Result == Expected`\n",
    "\n",
    "When you run an assert statement in python, one of 2 things will happen:\n",
    "1. Nothing\n",
    "2. An [AssertionError](https://docs.python.org/2/reference/simple_stmts.html#assert)\n",
    "\n",
    "**WARNING!** Assert statements are only as good as the person writing them! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Poor Assert Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/DSR_Model_Pipelines_Course/0.1 Introduction to Tests and Test Driven Development.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m  \u001b[0madd_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def add_1(x):\n",
    "    return x + 1\n",
    "\n",
    "assert  add_1(1) == 3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Assertion Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AssertionError we got there isn't very helpful.\n",
    "\n",
    "\n",
    "We can improve the output we get from the assert statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Adder wrong! Check arithmetic in function and test.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/DSR_Model_Pipelines_Course/0.1 Introduction to Tests and Test Driven Development.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0madd_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Adder wrong! Check arithmetic in function and test.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Adder wrong! Check arithmetic in function and test."
     ]
    }
   ],
   "source": [
    "assert add_1(1) == 3, \"Adder wrong! Check arithmetic in function and test.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated Tests\n",
    "===========\n",
    "You can write tests in python that run automatically.\n",
    "[pytest](https://docs.pytest.org/en/latest/) is an extremely useful python package for automated testing.\n",
    "\n",
    "Here, we will be using [ipytest](https://github.com/chmp/ipytest), which allows pytest to run in jupyter notebooks.\n",
    "\n",
    "\n",
    "First:   `pip install ipytest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest.magics\n",
    "import pytest\n",
    "# set the file name (required)\n",
    "__file__ = '0.1 Introduction to Tests and Test Driven Development.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/rachelberryman/Documents/DSR_Model_Pipelines_Course, inifile:\n",
      "collected 1 item\n",
      "\n",
      "0.1 Introduction to Tests and Test Driven Development.py .\n",
      "\n",
      "=========================== 1 passed in 0.03 seconds ===========================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest \n",
    "\n",
    "def test_tester():\n",
    "    assert 42 == 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Tests: Example with Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_x, boston_y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use tests to quickly check our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# checking that features and target have same number of records.\n",
    "assert boston_x.shape[0] == len(boston_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if NaNs exist in the data\n",
    "assert np.any(np.isnan(boston_x)) == False\n",
    "assert np.any(np.isnan(boston_y)) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Using TDD, write a simple regression model to predict the housing prices. \n",
    "\n",
    "Start with a test of what the outcome should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(boston_x, boston_y, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor()\n",
    "reg.fit(x_train, y_train)\n",
    "predictions = reg.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = reg.score(x_train, y_train)\n",
    "test_score = reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/rachelberryman/Documents/DSR_Model_Pipelines_Course, inifile:\n",
      "collected 2 items\n",
      "\n",
      "0.1 Introduction to Tests and Test Driven Development.py ..\n",
      "\n",
      "=========================== 2 passed in 0.04 seconds ===========================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest\n",
    "\n",
    "def test_predictions():\n",
    "    \n",
    "    # checking for data leakage\n",
    "    assert len(predictions) == len(y_test)\n",
    "    \n",
    "    # Sanity checks for our algorithm.\n",
    "    # Our train score should be higher than the test score.\n",
    "    # both scores should be under 100.\n",
    "    assert train_score > test_score\n",
    "    assert np.all([train_score, test_score]) < 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- http://python.kaihola.fi/static/unit-testing-scipy-applications.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
