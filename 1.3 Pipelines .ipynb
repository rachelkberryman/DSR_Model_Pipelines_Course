{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3: Pipelines\n",
    "================\n",
    "### Stringing it All Together\n",
    "_____________________\n",
    "\n",
    "**Pipelines** has a few different meanings in Data Science.\n",
    "\n",
    "This creates some ambiguity as the terms are often used interchangably.\n",
    "\n",
    "There's [Spark Pipelines](https://spark.apache.org/docs/2.2.0/ml-pipeline.html), used to process data in PySpark. \n",
    "\n",
    "Then there's the overarching idea of the Data Science Pipeline: a concept in business that describes the general process for going from raw data to something that creates value for the business, as this [example](https://www.ibm.com/developerworks/library/ba-intro-data-science-1/index.html) from IBM illustrates.\n",
    "\n",
    "Then, there is the scikit-learn Pipeline tool. This can be described as a simple way, using scikit-learn, to work through the beginning/middle steps of the Data Science Pipeline. **In scikit-learn, a Pipeline is a string of transforms with a final estimator.** All of these steps in the pipeline (transformers and estimators) can be user-defined or from scikit-learn.\n",
    "\n",
    "**You can make user-defined pipeline steps by inheriting from the TransformerMixin and BaseEstimator parent classes**. If you inherit from these and write your transformer or estimator properly, they will work together in the pipeline seamlessly!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Scikit-learn pipelines to the rescue\n",
    "-------------\n",
    "\n",
    "Fortunately scikit-learn provides a set of helpful functions to deal with pipelines.\n",
    "2 of them are the most important:\n",
    "\n",
    "1. `sklearn.pipeline.make_pipeline`\n",
    "\n",
    "    In our previous example we could define our transformer like this\n",
    "    \n",
    "```python\n",
    "adder_normalizer = make_pipeline(\n",
    "    AdderTransformer(add=10),\n",
    "    MeanNormalizer()\n",
    ")\n",
    "```\n",
    "Calling `fit` on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline.\n",
    "\n",
    "2. `sklearn.pipeline.make_union`\n",
    "\n",
    "    Creates a union of transformers\n",
    "    \n",
    "    ```\n",
    "    \n",
    "             transformer 1\n",
    "           /               \\\n",
    "          /                 \\\n",
    "    input                     output\n",
    "          \\                 /    \n",
    "           \\               /\n",
    "             transformer 2\n",
    "             \n",
    "    ```\n",
    "             \n",
    "    It is useful when the dataset consists of several types of data that one must \n",
    "    deal with separately.\n",
    "\n",
    "\n",
    "Alternative way to define pipelines\n",
    "--------------\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "adder_normalizer = Pipeline([\n",
    "    ('adder', AdderTransformer(add=10)),\n",
    "    ('normalizer', MeanNormalizer()),    \n",
    "])\n",
    "\n",
    "print(adder_normalizer)\n",
    "\n",
    ">> Pipeline(steps=[('adder', <__main__.AdderTransformer object at 0x7f9387473750>), ('normalizer', <__main__.MeanNormalizer object at 0x7f9387137e50>)])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with real data: Predicting Absenteeism at Work\n",
    "Here, we will take a real data set, and follow first the exploratory phase, then make it into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                   int64\n",
      "Reason for absence                  object\n",
      "Month of absence                     int64\n",
      "Day of the week                     object\n",
      "Distance from Residence to Work      int64\n",
      "Service time                         int64\n",
      "Age                                  int64\n",
      "Work load Average/day              float64\n",
      "Hit target                           int64\n",
      "Disciplinary failure                 int64\n",
      "Education                           object\n",
      "Number of Children                   int64\n",
      "Social drinker                       int64\n",
      "Social smoker                        int64\n",
      "Pet                                  int64\n",
      "Weight                               int64\n",
      "Height                               int64\n",
      "Body mass index                      int64\n",
      "Absenteeism time in hours            int64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Summary Statistics for Target Variable \n",
      " count    740.000000\n",
      "mean       6.924324\n",
      "std       13.330998\n",
      "min        0.000000\n",
      "25%        2.000000\n",
      "50%        3.000000\n",
      "75%        8.000000\n",
      "max      120.000000\n",
      "Name: Absenteeism time in hours, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Reason for absence</th>\n",
       "      <th>Month of absence</th>\n",
       "      <th>Day of the week</th>\n",
       "      <th>Distance from Residence to Work</th>\n",
       "      <th>Service time</th>\n",
       "      <th>Age</th>\n",
       "      <th>Work load Average/day</th>\n",
       "      <th>Hit target</th>\n",
       "      <th>Disciplinary failure</th>\n",
       "      <th>Education</th>\n",
       "      <th>Number of Children</th>\n",
       "      <th>Social drinker</th>\n",
       "      <th>Social smoker</th>\n",
       "      <th>Pet</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>Body mass index</th>\n",
       "      <th>Absenteeism time in hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>Patient follow-up</td>\n",
       "      <td>7</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>High school</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>172</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>No reason given</td>\n",
       "      <td>7</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>High school</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Blood donation</td>\n",
       "      <td>7</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>51</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>High school</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>170</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Diseases of the eye and adnexa</td>\n",
       "      <td>7</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>High school</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>168</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Blood donation</td>\n",
       "      <td>7</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>High school</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>172</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID              Reason for absence  Month of absence Day of the week  \\\n",
       "0  11               Patient follow-up                 7         Tuesday   \n",
       "1  36                 No reason given                 7         Tuesday   \n",
       "2   3                  Blood donation                 7       Wednesday   \n",
       "3   7  Diseases of the eye and adnexa                 7        Thursday   \n",
       "4  11                  Blood donation                 7        Thursday   \n",
       "\n",
       "   Distance from Residence to Work  Service time  Age  Work load Average/day   \\\n",
       "0                               36            13   33                 239.554   \n",
       "1                               13            18   50                 239.554   \n",
       "2                               51            18   38                 239.554   \n",
       "3                                5            14   39                 239.554   \n",
       "4                               36            13   33                 239.554   \n",
       "\n",
       "   Hit target  Disciplinary failure    Education  Number of Children  \\\n",
       "0          97                     0  High school                   2   \n",
       "1          97                     1  High school                   1   \n",
       "2          97                     0  High school                   0   \n",
       "3          97                     0  High school                   2   \n",
       "4          97                     0  High school                   2   \n",
       "\n",
       "   Social drinker  Social smoker  Pet  Weight  Height  Body mass index  \\\n",
       "0               1              0    1      90     172               30   \n",
       "1               1              0    0      98     178               31   \n",
       "2               1              0    0      89     170               31   \n",
       "3               1              1    0      68     168               24   \n",
       "4               1              0    1      90     172               30   \n",
       "\n",
       "   Absenteeism time in hours  \n",
       "0                          4  \n",
       "1                          0  \n",
       "2                          2  \n",
       "3                          4  \n",
       "4                          2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data/data.csv', index_col=0)\n",
    "print(data.dtypes)\n",
    "print('\\n')\n",
    "print('Summary Statistics for Target Variable \\n', data['Absenteeism time in hours'].describe())\n",
    "# we have a mix of categorical, numeric, and string data.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by writing some initial tests to check our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that we don't have any null values\n",
    "assert data.isnull().any().any() == False\n",
    "\n",
    "# test passes. No missing values exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we get an initial idea of the shape and form of our data?\n",
    "\n",
    "We can start by phrasing a few questions, and working to answer them.\n",
    "\n",
    "#### 1. What is the average amount of time for which employees are sick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.924324324324324"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Absenteeism time in hours'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is the average age of our employees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.45"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3. What are the most common reason that employees are absent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reason for absence\n",
       "Blood donation                                                  149\n",
       "Dental Consultation                                             112\n",
       "Physiotherapy                                                    69\n",
       "Diseases of the muskuloskeletal system and connective tissue     55\n",
       "No reason given                                                  43\n",
       "Name: Reason for absence, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['Reason for absence'])['Reason for absence'].count().sort_values(ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Write your own question and answer it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a rough idea of the data, we can start preparing it for modeling.\n",
    "At the very least, we need to:\n",
    "1. Separate the target from the features\n",
    "2. Split the data into test and train \n",
    "3. Encode the categorical features (also means separating them from the numeric features)\n",
    "4. Scale the numeric features\n",
    "5. Choose and apply a final estimator\n",
    "6. Calculate the score of the estimator\n",
    "\n",
    "**Which of these steps can we build into a pipeline?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-pipeline steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = data['Absenteeism time in hours']\n",
    "features = data.drop('Absenteeism time in hours', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Steps: \n",
    "### Step 1: Building the Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the numeric columns from the categorical ones:\n",
    "Start with a test! What does the outcome look like?\n",
    "We can start with the assert statement, and work backwards to developing a full test. \n",
    "\n",
    "\n",
    "```python\n",
    "# checking that when we fit & transform with the transformer, we only have numeric columns.\n",
    "for column in processed_df:\n",
    "    assert is_numeric_dtype(df[column])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also start by defining what columns are categorical and what columns are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Reason for absence', 'Month of absence', 'Day of the week',\n",
      "       'Distance from Residence to Work', 'Service time', 'Age',\n",
      "       'Work load Average/day ', 'Hit target', 'Disciplinary failure',\n",
      "       'Education', 'Number of Children', 'Social drinker', 'Social smoker',\n",
      "       'Pet', 'Weight', 'Height', 'Body mass index'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Some of the \"numeric\" columns are more categorical in nature (ex: month of absence, ID). We don't want to scale these as they are not continuous values. For this reason, we will split them out on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 features with categories represented by strings.\n",
    "There are 2 ways to deal with these: We can Label Encode them, essentially making them ordinal features (like ID already is). Or, we can one-hot encode them. This means making a column with binary values for each of the possible values of the feature.\n",
    "\n",
    "The one that will work best for our purposes will depend on how many features there are, and the shape of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making 28 new columns for all of the reasons of absence can start to lead to the \"curse of dimensionality', especially given that our data only has about 500 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(features['Reason for absence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(features['Education']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_columns = ['Distance from Residence to Work',\n",
    "                   'Service time',\n",
    "                   'Age',\n",
    "                   'Work load Average/day ',\n",
    "                   'Number of Children',\n",
    "                   'Weight', \n",
    "                   'Height',\n",
    "                   'Body mass index']\n",
    "\n",
    "binary_columns = ['ID',\n",
    "                  'Month of absence',\n",
    "                  #'Day of the week',\n",
    "                  'Disciplinary failure',\n",
    "                  'Hit target',\n",
    "                  'Social drinker', \n",
    "                  'Social smoker', \n",
    "                  'Pet']\n",
    "\n",
    "label_encode_column = ['Reason for absence']\n",
    "one_hot_encode_column = ['Education', 'Day of the week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipytest.magics\n",
    "import pytest\n",
    "# set the file name (required)\n",
    "__file__ = '1.3 Pipelines .ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test SHOULD fail! We haven't written the column selector yet!<br>\n",
    "Our goal is to make the test pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/rachelberryman/Documents/DSR_Model_Pipelines_Course, inifile:\n",
      "collected 1 item\n",
      "\n",
      "1.3 Pipelines .py F\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_____________________________ test_ColumnSelector ______________________________\n",
      "\n",
      "    def test_ColumnSelector():\n",
      "        # Here, showing a test case for both categoric and numeric datatypes.\n",
      ">       for column in numeric_df:\n",
      "E       NameError: name 'numeric_df' is not defined\n",
      "\n",
      "<ipython-input-14-005d543dfbd4>:4: NameError\n",
      "=========================== 1 failed in 0.15 seconds ===========================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest \n",
    "\n",
    "def test_ColumnSelector():\n",
    "    # Here, showing a test case for both categoric and numeric datatypes.\n",
    "    for column in numeric_df:\n",
    "        assert is_numeric_dtype(numeric_df[column]) == True\n",
    "    \n",
    "    for column in categoric_df:\n",
    "        assert is_numeric_dtype(categoric_df[column]) == False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        cols = x.loc[:,self.columns]\n",
    "        return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if we can make our test pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_selector = ColumnSelector(numeric_columns)\n",
    "numeric_df = numeric_selector.fit_transform(x_train)\n",
    "\n",
    "label_encode_selector = ColumnSelector(label_encode_column)\n",
    "label_encode_df = label_encode_selector.fit_transform(x_train)\n",
    "\n",
    "one_hot_encode_selector = ColumnSelector(one_hot_encode_column)\n",
    "one_hot_encode_df = one_hot_encode_selector.fit_transform(x_train)\n",
    "\n",
    "binary_selector = ColumnSelector(binary_columns)\n",
    "binary_df = binary_selector.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/rachelberryman/Documents/DSR_Model_Pipelines_Course, inifile:\n",
      "collected 1 item\n",
      "\n",
      "1.3 Pipelines .py .\n",
      "\n",
      "=========================== 1 passed in 0.02 seconds ===========================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest \n",
    "\n",
    "def test_ColumnSelector():\n",
    "    # Here, showing a test case for both categoric and numeric datatypes.\n",
    "    for column in numeric_df:\n",
    "        assert is_numeric_dtype(numeric_df[column]) == True\n",
    "    \n",
    "    assert is_numeric_dtype(label_encode_df) == False\n",
    "    assert is_numeric_dtype(one_hot_encode_df) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can write more tests for the additional transformers we want to add into our pipeline.\n",
    "<del>`class CategoricFeatureEncoder(BaseEstimator, TransformerMixin):`</del>\n",
    "\n",
    "we don't need to write our own class for this. Scikit-learn already has many built in. **When possible, don't reinvent the wheel. Use the transformers that are already there for you!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 2 different encoders in our pipeline. The feature \"Reason for absence\" has 28 distinct categories. This is too many to one-hot encode and would add too many features when our dataset only has 740 records.\n",
    "\n",
    "We can still write some tests to see that they're working correctly. We'll start by using our ColumnSelector transformer (from our tests above) to separate out the 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.ordinal import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Why are we not using sk-learn's [Label Encoder](), which works the same way as the [Ordinal Encoder](http://contrib.scikit-learn.org/categorical-encoding/ordinal.html) from the Category Encoders package? Because the sci-kit learn one (somehow) [doesn't work](https://github.com/scikit-learn/scikit-learn/issues/3112) with pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/rachelberryman/Documents/DSR_Model_Pipelines_Course, inifile:\n",
      "collected 2 items\n",
      "\n",
      "1.3 Pipelines .py ..\n",
      "\n",
      "=========================== 2 passed in 0.04 seconds ===========================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest \n",
    "\n",
    "def test_LabelEncoder():\n",
    "    encoder = OrdinalEncoder()\n",
    "    encoded_df = encoder.fit_transform(label_encode_df.values)\n",
    "    \n",
    "    # check for data leakage\n",
    "    assert encoded_df.shape[0] == label_encode_df.shape[0]\n",
    "    \n",
    "    # check that all values have been converted into integers\n",
    "    assert encoded_df.values.dtype == 'int64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write a test for the One-Hot Encoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double click to see the solution**\n",
    "\n",
    "<div class='spoiler'>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why are we not using the One-Hot Encoder from Scikit-Learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in /anaconda3/lib/python3.6/site-packages (1.2.8)\n",
      "Requirement already satisfied: pandas>=0.15.0 in /anaconda3/lib/python3.6/site-packages (from category_encoders) (0.22.0)\n",
      "Requirement already satisfied: scikit-learn>=0.15.0 in /anaconda3/lib/python3.6/site-packages (from category_encoders) (0.19.1)\n",
      "Requirement already satisfied: patsy>=0.4.0 in /anaconda3/lib/python3.6/site-packages (from category_encoders) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.8.0 in /anaconda3/lib/python3.6/site-packages (from category_encoders) (1.14.2)\n",
      "Requirement already satisfied: statsmodels<=0.8.0,>=0.6.0 in /anaconda3/lib/python3.6/site-packages (from category_encoders) (0.8.0)\n",
      "Requirement already satisfied: scipy>=0.9 in /anaconda3/lib/python3.6/site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.6/site-packages (from pandas>=0.15.0->category_encoders) (2017.2)\n",
      "Requirement already satisfied: python-dateutil>=2 in /Users/rachelberryman/.local/lib/python3.6/site-packages (from pandas>=0.15.0->category_encoders) (2.7.3)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from patsy>=0.4.0->category_encoders) (1.11.0)\n",
      "\u001b[31mboto3 1.5.33 has requirement botocore<1.9.0,>=1.8.47, but you'll have botocore 1.10.19 which is incompatible.\u001b[0m\n",
      "\u001b[31mbacnet-controller 0.0.1 has requirement boto3==1.5.12, but you'll have boto3 1.5.33 which is incompatible.\u001b[0m\n",
      "\u001b[31mbacnet-controller 0.0.1 has requirement pytz==2018.3, but you'll have pytz 2017.2 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.one_hot import OneHotEncoder\n",
    "one_hot = OneHotEncoder()\n",
    "one_hot_encoded_df = one_hot.fit_transform(one_hot_encode_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/rachelberryman/Documents/DSR_Model_Pipelines_Course, inifile:\n",
      "collected 1 item\n",
      "\n",
      "1.3 Pipelines .py .\n",
      "\n",
      "=========================== 1 passed in 0.06 seconds ===========================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_OneHotEncoder():\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    one_hot_encoded_df = one_hot.fit_transform(one_hot_encode_df.values)\n",
    "    \n",
    "    # check for data leakage\n",
    "    assert one_hot_encoded_df.shape[0] == one_hot_encode_df.shape[0]\n",
    "    \n",
    "    # check that all values have been converted into integers\n",
    "    assert one_hot_encoded_df.dtypes.all() == 'int64'\n",
    "    \n",
    "    # check that only 0s and 1s exist in the new matrix\n",
    "    assert ((one_hot_encoded_df.values ==0) | (one_hot_encoded_df.values ==1)).all()\n",
    "    \n",
    "    # check that a dummy column has been made for each potential category \n",
    "    assert one_hot_encoded_df.shape[1] == len(set(one_hot_encoded_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Numeric Data: Using Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance from Residence to Work</th>\n",
       "      <th>Service time</th>\n",
       "      <th>Age</th>\n",
       "      <th>Work load Average/day</th>\n",
       "      <th>Number of Children</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>Body mass index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>261.306</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>169</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>265.017</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>172</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>51</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>251.818</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>170</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>343.253</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>168</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>43</td>\n",
       "      <td>294.217</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>167</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Distance from Residence to Work  Service time  Age  \\\n",
       "108                               26             9   28   \n",
       "310                               52             3   28   \n",
       "377                               51            18   38   \n",
       "156                               50            11   36   \n",
       "273                               20            13   43   \n",
       "\n",
       "     Work load Average/day   Number of Children  Weight  Height  \\\n",
       "108                 261.306                   1      69     169   \n",
       "310                 265.017                   1      80     172   \n",
       "377                 251.818                   0      89     170   \n",
       "156                 343.253                   4      65     168   \n",
       "273                 294.217                   1     106     167   \n",
       "\n",
       "     Body mass index  \n",
       "108               24  \n",
       "310               27  \n",
       "377               31  \n",
       "156               23  \n",
       "273               38  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_numeric = scaler.fit_transform(numeric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, to Pipelines!\n",
    "We've tested our transformers and are sure they are working. What we need to do now is string them together in a pipeline, and re-combine the various features.\n",
    "\n",
    "Without a pipeline, we would have had to do this by merging all or our new features. \n",
    "This is difficult, since after scaling, the numeric features are a matrix. The binary and categoric features are still dataframes.\n",
    "A pipeline, combined with another scikit-learn tool `make_union`, does all of this work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_columns = list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "\n",
    "# process the numeric and categorical columns.\n",
    "# then, join them all together.\n",
    "\n",
    "processing_pipeline = make_pipeline(\n",
    "    ColumnSelector(all_columns),\n",
    "    make_union(\n",
    "        # First, we select and 'hold out' the binary variables, as we wont do any further work to them.\n",
    "        make_pipeline(ColumnSelector(binary_columns),\n",
    "        ),\n",
    "        # Pipeline for numeric features\n",
    "        make_pipeline(\n",
    "            ColumnSelector(numeric_columns),\n",
    "            StandardScaler()\n",
    "        ),\n",
    "        # Pipeline for label encoded features\n",
    "        make_pipeline(\n",
    "            ColumnSelector(label_encode_column),\n",
    "            OrdinalEncoder()\n",
    "        ),\n",
    "        \n",
    "        # Pipeline for one-hot-encoded features\n",
    "        make_pipeline(\n",
    "            ColumnSelector(one_hot_encode_column),\n",
    "            OneHotEncoder()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = processing_pipeline.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's write a test to make sure that we didn't lose any data when we strung all of our transformers and estimators together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\n",
      "rootdir: /Users/rachelberryman/Documents/DSR_Model_Pipelines_Course, inifile:\n",
      "collected 1 item\n",
      "\n",
      "1.3 Pipelines .py .\n",
      "\n",
      "=========================== 1 passed in 0.06 seconds ===========================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_processingpipeline():\n",
    "    # remember, this first pipeline only acts on the features, not the target.\n",
    "    processed = processing_pipeline.fit_transform(features)\n",
    "    \n",
    "    # check for data leakage\n",
    "    assert features.shape[0] == processed.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, choosing the final estimator.\n",
    "We can use our estiamtor tests we wrote in the beginning of class, when we learned about tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting random seed for reproducability. \n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(verbose=True, n_jobs=-1)\n",
    "final_pipeline = (make_pipeline(processing_pipeline, rfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "pred = cross_val_predict(final_pipeline, \n",
    "                         x_train, \n",
    "                         y_train,\n",
    "                         cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237.3984053226696\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_train, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
