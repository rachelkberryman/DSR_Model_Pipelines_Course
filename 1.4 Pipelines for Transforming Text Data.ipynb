{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise made by Pavel Jankiewicz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "}*/\n",
       "\n",
       ".navbar-brand, .current_kernel_logo {display:none}\n",
       ".container {\n",
       "    width:80%;    \n",
       "}\n",
       "\n",
       "h1 {\n",
       "\tfont-family: Helvetica, serif;\n",
       "}\n",
       "h4{\n",
       "\tmargin-top:12px;\n",
       "\tmargin-bottom: 3px;\n",
       "   }\n",
       "div.text_cell_render{\n",
       "\tfont-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "\tline-height: 145%;\n",
       "\tfont-size: 100%;\n",
       "\twidth:100%;\n",
       "\tmargin-left:auto;\n",
       "\tmargin-right:auto;\n",
       "}\n",
       ".CodeMirror{\n",
       "\t\tfont-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "\tfont-weight: 300;\n",
       "\tfont-size: 22pt;\n",
       "\t/*color: #4057A1;*/\n",
       "\tfont-style: italic;\n",
       "\tmargin-bottom: .5em;\n",
       "\tmargin-top: 0.5em;\n",
       "\tdisplay: block;\n",
       "}\n",
       "\n",
       ".warning{\n",
       "\tcolor: rgb( 240, 20, 20 )\n",
       "\t}   \n",
       "\n",
       "div.spoiler {\n",
       "\tdisplay: none;\n",
       "}\n",
       "\n",
       ".rendered_html code {\n",
       "\tborder: 0;\n",
       "\t/*background-color: #eee;*/\n",
       "\tfont-size: 100%;\n",
       "\tpadding: 1px 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import css_from_file\n",
    "css_from_file('style/style.css')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>896197</th>\n",
       "      <td>Tube Lined Victorian Style Green Garland Firep...</td>\n",
       "      <td>Home, Furniture &amp; DIY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349617</th>\n",
       "      <td>Cotton Round Funky Retro Bathroom Mat Blue Bla...</td>\n",
       "      <td>Home, Furniture &amp; DIY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225735</th>\n",
       "      <td>brand new with tags mens DOCKERS the oxford sh...</td>\n",
       "      <td>Clothes, Shoes &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374192</th>\n",
       "      <td>Lavender - Ellagance Series Snow - 20 Seeds</td>\n",
       "      <td>Garden &amp; Patio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81984</th>\n",
       "      <td>2m PS2 Keyboard / Mouse Extension Cable 2 metr...</td>\n",
       "      <td>Computers/Tablets &amp; Networking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "896197  Tube Lined Victorian Style Green Garland Firep...   \n",
       "349617  Cotton Round Funky Retro Bathroom Mat Blue Bla...   \n",
       "225735  brand new with tags mens DOCKERS the oxford sh...   \n",
       "374192        Lavender - Ellagance Series Snow - 20 Seeds   \n",
       "81984   2m PS2 Keyboard / Mouse Extension Cable 2 metr...   \n",
       "\n",
       "                         category_name  \n",
       "896197           Home, Furniture & DIY  \n",
       "349617           Home, Furniture & DIY  \n",
       "225735    Clothes, Shoes & Accessories  \n",
       "374192                  Garden & Patio  \n",
       "81984   Computers/Tablets & Networking  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ebaytitles.csv\")\n",
    "df = df.sample(frac=0.1) # delete this line if you are brave and have many GBs of RAM\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out unique values of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Home, Furniture & DIY', 'Clothes, Shoes & Accessories',\n",
       "       'Garden & Patio', 'Computers/Tablets & Networking',\n",
       "       'Health & Beauty', 'Mobile Phones & Communication',\n",
       "       'Sound & Vision', 'Vehicle Parts & Accessories',\n",
       "       'Business, Office & Industrial', 'Sporting Goods',\n",
       "       'Jewellery & Watches', 'Music', 'Collectibles', 'Art',\n",
       "       'Toys & Games', 'Dolls & Bears', 'Crafts', 'DVDs, Films & TV',\n",
       "       'Books, Comics & Magazines', 'Musical Instruments & Gear',\n",
       "       'Coins & Paper Money', 'Pottery, Porcelain & Glass', 'Baby',\n",
       "       'Cameras & Photography', 'Everything Else', 'Pet Supplies',\n",
       "       'Sports Memorabilia', 'Video Games & Consoles',\n",
       "       'Consumer Electronics', 'Antiques', 'Wholesale & Job Lots',\n",
       "       'Stamps', 'Cell Phones & Accessories', 'Travel',\n",
       "       'Entertainment Memorabilia', 'Holidays & Travel', 'Events Tickets'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test observations - there is a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.title.values\n",
    "y = df.category_name.values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, \n",
    "                                          y,\n",
    "                                          test_size=0.1,\n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise \n",
    "------------------\n",
    "\n",
    "1. Count how many titles are in each category (```pandas.DataFrame.groupby```). Print out most common at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# put your solution here #\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "frequencies = df.groupby(\"category_name\")[\"title\"].count()\n",
    "frequencies.sort_values(inplace=True,ascending=False)\n",
    "print(frequencies)\n",
    "\n",
    "# or faster\n",
    "\n",
    "df.category_name.value_counts()\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "--------------------\n",
    "\n",
    "Different types of vectorizers:\n",
    "\n",
    "<ul>\n",
    "<li>```sklearn.feature_extraction.text.CountVectorizer``` - Counts the number of times a word appears in the text</li>\n",
    "<li>```sklearn.feature_extraction.text.TfidfVectorizer``` - Weighs the words according to the importance of the word in the context of whole collection. Is the word ```the``` important if it appears in all documents?</li>\n",
    "<li>```sklearn.feature_extraction.text.HashingVectorizer``` - Useful when you don't know the vocabulary upfront. Feature number is calculated as ```hash(token) % vocabulary_size```.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "-------------------\n",
    "1. Use ```CountVectorizer``` / ```TfidfVectorizer``` to fit the collection of documents\n",
    "2. How many unique tokens are there in text? Print some examples (ie first few hundred).\n",
    "3. What methods you can use to reduce this number? \n",
    "   - Check out and experiment with the arguments: ```ngram_range```, ```min_df```. How the vocabulary size changes with each change?\n",
    "   - What would you replace / delete from the text?\n",
    "4. Write a custom function `clean_text` that accepts a text as input and transforms it (remove/hash numbers, delete short/long words etc.)\n",
    "5. (Extra points) When would you use ```HashingVectorizer```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# put your solution here #\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "vectorizers = [\n",
    "     (\"vanilla\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming\n",
    "------------------\n",
    "\n",
    "Linguistic normalization in which variant forms are reduced to a common form\n",
    "\n",
    "    connection\n",
    "    connections\n",
    "    connective     --->   connect\n",
    "    connected\n",
    "    connecting\n",
    "    \n",
    "Usage:\n",
    "\n",
    "    import snowballstemmer\n",
    "\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    print(stemmer.stemWords(\"We are the world\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it into a pipeline\n",
    "----------------------\n",
    "\n",
    "Now that we know how to transform text data, let's put it into a pipeline.\n",
    "\n",
    "1. Create a pipeline with `CountVectorizer`, `StandardScaler` and `SGDClassifier` as your final algorithm\n",
    "    a) use alternative format for pipeline definition when you name the steps - refer to the documentation how to do this\n",
    "2. Using ```sklearn.metrics.classification_report``` create a report about your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# put your solution here #\n",
    "##########################\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline()\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = make_pipeline(CountVectorizer(), SGDClassifier())\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('sgd', SGDClassifier())\n",
    "])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds)\n",
    "\n",
    "clf.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', SGDClassifier())])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds))\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search\n",
    "--------------------------\n",
    "\n",
    "Scikit-learn has `GridSearchCV` and `RandomizedSearchCV`. Both have the same functionality and can be used to find good parameters for the models. What is great about both these classes that they are both transformers - they return an estimator so you can chain them and put in your pipeline.\n",
    "\n",
    "**GridSearchCV** - you specify the exact values of the parameters you want to test\n",
    "**RandomizedSearchCV** - you specify ranges of parameters\n",
    "\n",
    "Exercise\n",
    "----------------------\n",
    "\n",
    "1. Use `GridSearchCV` or `RandomizedSearchCV` to find the best parameters for the models. Check at least 2 parameters.\n",
    "\n",
    "2. Inspect the attribute `cv_results_` after fitting. It gives a nice representation of the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# put your solution here #\n",
    "##########################\n",
    "\n",
    "GridSearchCV(pipeline, {'sgd__alpha':[0.1,0.01,0.001]\n",
    "                        'vect__min_df':[1,5,10,25],\n",
    "                        'vect__ngram_range'[(1,1),(1,2),(1,3)]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'vect__binary': [True, False]}\n",
    "\n",
    "grid_clf = GridSearchCV(clf, params, n_jobs=1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "    \n",
    "print(\"Randomized search\")\n",
    "print()\n",
    "    \n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'model__lr__dimensions': [100, 200]}\n",
    "\n",
    "grid_clf = RandomizedSearchCV(clf, params, n_jobs=1, verbose=True, n_iter=8)\n",
    "grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful materials\n",
    "\n",
    "1. http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
