{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling pipelines\n",
    "=================\n",
    "SOURCE: Pavel Jankiewicz<br>\n",
    "Thinking about modeling as a series of transformations is really helpful.\n",
    "Pipelines and functional transformations are the cleanest way to preprocess the data.\n",
    "It has its roots in Category theory from mathematics.\n",
    "\n",
    "Functional transformers are reusable and you can create many complicated things with them (think about Lego blocks).\n",
    "\n",
    "Assumptions\n",
    "-------------------\n",
    "\n",
    "1. We will be using scikit-learn interface to pipelines.\n",
    "2. We will use pandas dataframes as inputs to pipelines (useful).\n",
    "\n",
    "There are 2 types of building blocks of machine learning pipelines: transformers and estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory\n",
    "--------------------\n",
    "\n",
    "There is another name for the type of operations will be doing today.\n",
    "\n",
    "All of the pipeline transformations are just functions.\n",
    "\n",
    "They are defined as an operation $\\cdot$ such that \n",
    "\n",
    "$f(a: S) \\rightarrow b: S$\n",
    "\n",
    "What should be true that a transformation $f$ changes $a$ to $b$ but the type of $a$ and $b$ is the same. \n",
    "\n",
    "So if your transformation:\n",
    "- accepts a matrix it should return a matrix\n",
    "- accepts a dataframe it should return a dataframe\n",
    "- accepts a json object is should return a json object\n",
    "\n",
    "```\n",
    "\"It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures.\" â€”Alan Perlis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers\n",
    "---------\n",
    "\n",
    "Blocks that have input and output and can be chained with other transformers.\n",
    "\n",
    "For example\n",
    "\n",
    "```\n",
    "Data -> [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] -> Output\n",
    "```\n",
    "\n",
    "`[ Select variables ]` - transformer for selecting variables\n",
    "\n",
    "`[ Normalize ]` - normalization step\n",
    "\n",
    "`[ Reduce dimensions ]` - dimension reduction\n",
    "\n",
    "\n",
    "-------------------\n",
    "\n",
    "Because every transformer has the same type of data as input and output altogether they \n",
    "also form a transformer.\n",
    "\n",
    "```\n",
    "Input -> [ [ Select variables ] -> [ Normalize ] -> [ Reduce dimensions ] ] -> Output\n",
    "\n",
    "Input -> [               Data preprocessing transformation                ] -> Output\n",
    "```\n",
    "\n",
    "-------------------\n",
    "\n",
    "An example of transformer that does nothing\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LazyTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x\n",
    "```\n",
    "\n",
    "-------------------\n",
    "\n",
    "Notice that there are 2 methods:\n",
    "\n",
    "1. **fit** - learns the information about the data - it becomes a stateful transformer\n",
    "2. **transform** - applies the transformation \n",
    "\n",
    "There are 2 types of transformers:\n",
    "1. **stateful** - they learn something when calling fit method\n",
    "2. **stateless** - they don't learn anything\n",
    "\n",
    "**Why stateless transformers are useful?**\n",
    "\n",
    "Transformers that don't need historical data to learn can be used in a type of learning\n",
    "called `online learning`. This type of learning fits pipelines beacuse it is an algorithm\n",
    "that uses the stream of observations to learn.\n",
    "\n",
    "It doesn't keep the history so there would be no way to use stateful transformers.\n",
    "\n",
    "\n",
    "** Some rules when writing transformers:**\n",
    "- Think about your outputs first: what is the end-goal?\n",
    "- Then: Work backwards from that end goal, to what you're starting with. Chances are this will save you a lot of time and make your code much smoother and easier to read and understand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "--------------\n",
    "\n",
    "1. Write a transformer that adds some number to the input, the number that is added should be passed in `__init__`\n",
    "2. Write a transformer that normalizes the input:\n",
    "   - in the fit method you must save the column means\n",
    "3. Combine these 2 transformers into a pipeline:\n",
    "   - hint: write a class that accepts list of transformers as argument\n",
    "   \n",
    "HINTS: All transformers are classes! All classes must have an `__init__` function. All transformers must inherit from the BaseEstimator and TransformerMixin parent classes. All transformers must have `fit` and `transform` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# answer - start\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AdderTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "\n",
    "class MeanNormalizer(TransformerMixin): \n",
    "\n",
    "    \n",
    "class TransformerPipeline(TransformerMixin):\n",
    "\n",
    "\n",
    "# answer - end\n",
    "\n",
    "# tests\n",
    "X = np.ones((10,10))\n",
    "adder = AdderTransformer(add=1)\n",
    "assert np.all(adder.transform(X) == X + 1), \"Adder transformer wrong\"\n",
    "\n",
    "X = np.ones((10,10))\n",
    "normalizer = MeanNormalizer()\n",
    "assert np.allclose(normalizer.fit_transform(X), np.zeros((10,10))), \"Mean normalizer wrong\"\n",
    "\n",
    "double_adder = TransformerPipeline([AdderTransformer(add=1), \n",
    "                                    AdderTransformer(add=2)])\n",
    "\n",
    "assert np.allclose(double_adder.transform(X), X+3), \"TransformerPipeline wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double click to see the solution**\n",
    "\n",
    "<div class='spoiler'>\n",
    "\n",
    "class AdderTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, add=0):\n",
    "        self.add = add\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x + self.add\n",
    "    \n",
    "class MeanNormalizer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, add=0):\n",
    "        self.add = add\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        self.means = x.mean(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x - self.means    \n",
    "    \n",
    "class TransformerPipeline(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, transformers):\n",
    "        self.transformers = transformers\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        x_ = x.copy()\n",
    "        for transformer in self.transformers:\n",
    "            x_ = transformer.fit_transform(x_)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, x):\n",
    "        x_ = x.copy()\n",
    "        for transformer in self.transformers:\n",
    "            x_ = transformer.transform(x_)\n",
    "        return x_\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
